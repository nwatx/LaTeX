\documentclass{scrreprt}
\usepackage{standalone}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphbox} % 1120
\usepackage{chez}

\title{Vector Calculus -- Honors}
\author{Neo Wang\\ Lecturer: William Bekcner}
\date{Last Updated: \today}

\begin{document}
\maketitle
\tableofcontents


\chapter{Vectors}

\section{Vector Basics}

\subsection{Lecture--August 22, 2022}

Riemann integrals deal with functions that are basically continuous.
You should use the notation $(x, y, z)$ or $x\hat{i} + y\hat{j} + z\hat{k}$

There are several coordinates: Cartesian, cylindrical, and spherical.

Spherical coordinates are given by $(r, \theta, \phi)$

\begin{example}
	Let $f$ by a continuous functions. Suppose $f(x,y,z)=g(\sqrt{x^2+y^2+z^2})$.

	Let 1. $f(x)=g(\sqrt{x^2+y^2+z^2})$
	and 2. $f(x, y, z) = h_1(\abs{x})h_2(\abs{y})+h_3(\abs{z})$

	how many such functions satisfy this?
\end{example}

\begin{definition}
	Some useful integrals
	\begin{itemize}
		\item Continuous: $dxdydz$
		\item Cylindrical: $rdrd\theta$
		\item Spherical: $r^2\sin\theta drd\theta d\phi$
	\end{itemize}
\end{definition}

\begin{definition}[Vectors]
	Cross product: $\vec{x}\wedge \vec{y} = -\hat{y} \wedge \hat{x}$ is a vector operation

	Dot product: $\vec{x}\cdot \vec{y} = \sum x_i y_i$ is a scalar operation
\end{definition}

\subsection{Lecture--August 23, 2022}

\begin{definition}[Coordinate Systems]

	$\vec{v}=(x,y,z)\rightarrow \vec{x}=(x_1,x_2,x_3)$

	Sometimes will not include $\vec{}$ symbol--we want to think more abstractly in order to build to higher concepts.
	Spherical coordinates: $(r,\theta,\phi)$, and $\theta$ is always the polar angle (from $z$-axis).

	Orientation is the order of $(x, y, z)$ which comes into play with \boxed{\text{change of variables.}}

\end{definition}

\begin{remark}[Property of the determinant]
	The determinant is always $+$ or $-$
\end{remark}

There are several volume form differentials:
\[
	dxdydz = r^2dr\sin \theta d\theta d\phi
\]
and note that $\sin \theta$ is always positive

\begin{remark}[Normal model]
	Normal distribution pdf: \[p(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^2}\]
	With $\mu=0$ and $\sigma=0$
	The error function is the simplest example of a function that is ``not integrable in elementary terms''

	\begin{align*}
		2 \int_0^\infty e^{-x^2}dx & = \sqrt{\pi}
	\end{align*}

	There are connections with physics i.e. the uncertainty principle and the quantum mechanics harmonic oscillator.

	\begin{align*}
		A         & = \int_{-\infty}^{\infty} e^{-x^2}dx                                  \\
		A^2       & = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} e^{-x^2 - y^2} dydx \\
		          & = \int_{0}^{2\pi}\int_{0}^{\infty} e^{-r^2}rdrd\theta                 \\
		          & = \frac{1}{2}\int_{0}^{2\pi} \int_{\infty}^{0} e^{-u}dud\theta        \\
		          & = \frac{1}{2}\int_{0}^{2\pi} 1 d\theta                                \\
		          & = \pi                                                                 \\
		\Aboxed{A & = \sqrt{\pi}}
	\end{align*}
\end{remark}

Differentials tell you how to compute an integral.

\begin{enumerate}
	\item The integral is linear (also the derivative)
	      \[
		      \int(af+bg)dm=a\int fdm + b\int gdm
	      \]
	\item For a non-negative function $f(x)\geq 0$ any way that you can calculate a
	      finite value for the integral gives you the ``correct answer.''
	\item Dilation: \[
		      \int_0^\infty f(ax)dx=\frac{1}{a}\int_0^\infty f(x)dx
	      \]
\end{enumerate}

\begin{remark}[For proving estimates for the dot or scalar product]

	Estimate: $\abs{x\cdot y}=\abs{\sum x_i y_i}\leq||\vec{x}||||\vec{y}||, x =(x_1,x_2,x_3), y=(y_1,y_2,y_3)$

	3 simple arguments:
	\begin{enumerate}
		\item Euclidian geometry
		\item Arithmetic
		\item Adding a variable $\leftarrow$ the best way and expands view to another parameter
	\end{enumerate}
\end{remark}
Properties of vectors:
vector products (may be a scalar $\vec{x}\cdot \vec{y}$ or a vector $\vec{x}\wedge \vec{y}$) and representation of data in terms of partial derivatives.

\subsection{Discussion--August 24, 2022}

Vectors are a directed line segment. A vector in $n$-dimensional space is an
ordered tuple of $n$ real numbers. A vector is denoted $\vec{v}=(a_1,\ldots, a_n), a_i \in \mathbb{R}$

The basic operations:

\begin{itemize}
	\item Addition of vectors: $\vec{v}+\vec{w}=(a_1+b_1,\ldots, a_n+b_n)$
	\item Multiplication by scalar: $c\vec{v}=(ca_1,\ldots, ca_n)$
\end{itemize}

Properties:

\begin{itemize}
	\item $\forall \vec{u}\in \mathbb{R}^n \langle \vec{u}, \vec{u}\rangle \geq 0$
	\item $\langle u, v \rangle = \langle v, u \rangle$
	\item Dot product is linear: \[
		      c(u\cdot v) = (cu)\cdot v = u\cdot (cv)
	      \]
\end{itemize}

\subsection{Lecture--August 25, 2022}

How to think about data: strings, words, ``columns'', rectangular arrays.

We can think of strings as slots: $(x_1, x_2, x_3, \ldots, x_n)$

Structures of organization may give insightful information on how to extra information

\begin{definition}[Triangle Inequality]
	The length of any side is less than the sum of the lengths of the other two sides.
\end{definition}

More abstract setting--use ``norm.'' Vectors: objects that we can add or subtract scalar mltiples--scalars.

Length--norm: $\abs{\vec{v}}=\sqrt{\langle \vec{v}, \vec{v}\rangle}$

\begin{definition}[Homogenous]
	A real-valued function $h(x)$ is homogenous of degree $\lambda$
	if $h(\lambda x)=\delta^\lambda h(x)$ where $\lambda \in \mathbb{R}$

	The differential $r^2drsin\theta d\theta d\phi$ is homogenous of degree $3$.

	This defines a vector space.
\end{definition}

\begin{remark}[Most important property from linear algebra]
	Suppose you have a finite collection of vectors which you want to be \fbox{linearly independent.}
	Then you can find a basis for the space spanned by the vectors. Then:
	\[
		c_1u_1 + c_2u_2 + \ldots + c_nu_n = 0 \Leftrightarrow c_1=c_2=\ldots=c_n=0
	\]

	in $\mathbb{R}^3$, the unit vectors $\vec{i}, \vec{j}, \vec{k}$ are linearly independent.
	Any vector expression written in terms of these vectors is unique. The only way
	for us to get the zero vector is if all the coefficients are zero.

\end{remark}

A vector space with a norm is called a ``normed vector space.''

\begin{example}[Normed vector space]
	Consider the set of continuous functions defined on the unit square--with scalars
	as real numbers, they are vector space. Restrict to all such functions that are
	square integrable on the unit square:

	\[
		\int_0^1\int_0^1|f(x,y)|^2dx dy < \infty
	\]

	if this is true then we can create a norm on this space by setting

	\[
		||f|| = \sqrt{\int_0^1\int_0^1|f(x,y)|^2dxdy}
	\]

	\[
		||f||_2 = \int_0^1\int_0^1|f(x,y)|dxdy
	\]

	Then we have informally said that $f$ is Lebesgue integrable.
\end{example}

\begin{remark}
	The function is defined for a domain and the function takes various values. What do you want to
	partition? You want to partition the range and you can estimate the function in order to get the
	Lebesgue integral. We can look at this fundamental difference via the diagram (Lebesgue on bottom):

	% lesbesgue diagram
	\includegraphics[width=300px]{figures/2022-08-25-11-47-29.png}
\end{remark}

\begin{fact}[Dot Product]
	$\vec{x}\cdot \vec{x} = \abs{\vec{x}}^2$
\end{fact}

\section{Cauchy Schwarz Inequality}

\begin{theorem}[Cauchy Schwartz Inequality]
	With respect to length--to show that for two vectors $\vec{x} = (x_1, x_2, x_3)$ and $\vec{y} = (y_1, y_2, y_3)$ then
	\[
		\abs{\vec{x}\cdot \vec{y}} \leq |\vec{x}| |\vec{y}|.
	\]

	same argument will work for abstract vector spaces with norm and scalar product.

	Multiple proofs: Euclidean geometry, arithmetic, to see the role of length. But proof should capture the spirit of calculating length.

	\begin{proof}
		We will add a parameter $\lambda$

		\[
			|\vec{x} - \lambda \vec{y}|
		\]

		Remember that lengths and norms have corners. These are not smooth.
		For example $f(x=|x|)$ is not smooth at $x=0$. You would like to smooth it
		out to something like $g(x)=x^2$. Therefore, square the expression to remove the ``corners.''

		Assume that $\vec{x}$ and $\vec{y}$ are non-zero. Otherwise nothing to show.

		\begin{align*}
			0 \leq |\vec{x}-\lambda \vec{y}|^2 & = \vec{x}\cdot \vec{x} + \lambda^2\vec{y}\cdot \vec{y} - 2\lambda\vec{x}\cdot \vec{y} \\ &= \abs{\vec{x}}^2 + \lambda^2\abs{\vec{y}}^2 - 2\lambda\vec{x}\cdot \vec{y} \\
			                                   & = |y|^2 + \brackets{\lambda^2-2B\lambda+C}                                            \\
			                                   & = \lambda^2 - 2B\lambda + C
		\end{align*}

		If we complete the square we get $\lambda^2-2B\lambda + B^2 + C - B^2\geq 0$

		\[
			(\lambda - B)^2 + C - B^2 \geq 0 \implies \boxed{C-B^2\geq 0}
		\]

		\[
			C = \frac{|x|^2}{|y|^2}, B = \frac{x\cdot y}{|y|^2}
		\]

		Expanding this, we get the Cauchy Schwartz inequality.
	\end{proof}
\end{theorem}

We want to extend length of vectors to norms, but also the dot product to scalar products.

\begin{itemize}
	\item $\vec{x}\cdot \vec{y}\rightarrow \angles{x, y}$ (scalar product)
	\item $\angles{x,y}=\angles{y,x}$ symmetry
	\item $\angles{x,\alpha y + \beta z} = \alpha\angles{x, y} + \beta \angles{x, z}$
	\item $\angles{x,x} \geq 0$ (positive definite)
	\item $\angles{x,x} = \norm{x}^2$
\end{itemize}

\subsection{Discussion--August 29, 2022}

\begin{definition}[Cross Product]
	\[
		\vec{a} = (x_1, y_1, z_1), \vec{b} = (x_2, y_2, z_2)
	\]

	\[
		\vec{a}\times \vec{b} = \det{\begin{bmatrix} i & j & k \\ x_1 & y_1 & z_1
                \\ x_2 & y_2 & z_2\end{bmatrix}}
		= \vec{i}\begin{bmatrix}
			y_1 & z_1 \\
			y_2 & z_2
		\end{bmatrix} - \vec{j}\begin{bmatrix}
			x_1 & z_1 \\
			x_2 & z_2
		\end{bmatrix} + \vec{k}\begin{bmatrix}
			x_1 & y_1 \\
			x_2 & y_2
		\end{bmatrix}
	\]

	$\forall \vec{a}, \vec{b}, \vec{c}\in \mathbb{R}^3$

	\begin{enumerate}
		\item $\vec{a}\times \vec{b} = -\vec{b}\times \vec{a}$
		\item Cross product is linear:
		      \begin{itemize}\item $\vec{a}\times (\vec{b} + \vec{c}) = \vec{a}\times \vec{b} + \vec{a}\times \vec{c}$
			      \item $\vec{a}\times \alpha \vec{b} = \alpha \vec{a}\times \vec{b}$
			      \item $(\vec{a}+\vec{b})\times \vec{c} = \vec{a}\times \vec{c} + \vec{b}\times \vec{c}$
		      \end{itemize}
		\item $\vec{a}\times \vec{a} = 0$
		\item $i\times j = k, j\times k = i, k\times i = j$ in order $i\rightarrow j \rightarrow k\rightarrow i, \ldots$
	\end{enumerate}
\end{definition}

\begin{example}
	Let $P = (-2, 1, 2), Q = (0, 0, 5), R = (5, 7, -1)$

	First, $\vec{u} = Q - P = (2, -1, 3)$ and $\vec{w} = R - P = (7, 6, -3)$

	\begin{enumerate}
		\item What is the area of PQR? $\frac{1}{2} \norm{u\times w}$
		\item What is the equation of a plane defined by P, Q, R?

		      $u\times w = \langle -15, 27, 19 \rangle$ is the normal vector to the plane

		      $\therefore (v - p) \cdot (u\times w) = -15(x + 2) + 27(y - 1) + 19(z-2) = 0$ is the equation of the plane
	\end{enumerate}
\end{example}

\subsection{Lecture--August 30, 2022}

Error Function, \[ \text{erf}(x) = \frac{2}{\sqrt{\pi}}\int_0^x e^{-\frac{1}{2}t^2}dt \]
$\text{erf}(0) = 0, \text{erf}(\infty) = 1$

Another function with similar character comes from the simplest 2nd order linear
differential equation with non-constant coefficients.

\[
	y''+xy'+y=0
\]

Since it's a second order equation there are 2 linearly independent solutions and

\[
	y_1(x)=c_1e^{-\frac{1}{2}x^2}, y_2(x) = c_2e^{-\frac{1}{2}x^2}\int_0^xe^{\frac{1}{2}t^2}dt
\]

Notice that $y_1$ is an even function, and that $y_2$ is an odd function. You should
expect this solution to have one even, one odd.

\[
	\brackets*{\int f(x)g(x)dm}^2 + \int \int_{x < y}|f(x)g(y)-f(y)g(x)|^2 du dm
	= \int |f(x)|^2dm\int \abs*{g(y)}^2dm
\]

\section{Differentiation and Linear Transformations}

Most important concept from calculus is the notion of a limit. The 1st application
of this is the definition of a derivative.
\[
	\lim_{h\rightarrow 0}\frac{f(x+h)-f(x)}{h} \rightarrow \text{does this limit exist?}
\]
The expression has a removal singularity at $h=0$.
\begin{example}[Classic Example]
	\[
		\frac{\sin x}{x} = \frac{1}{x}\sum_{k=0}^\infty \frac{(-1)^k}{(2k+1)!}x^{2k+1}
		= \sum_{k=0}^\infty \frac{(-1)^k}{(2k+1)!}x^{2k}
	\]
\end{example}
because of the way the factorial function grows, this converges everywhere.
Therefore, \[ \int_0^\infty \frac{sin x}{x} dx = \frac{\pi}{2} \]

Write as \[
	\lim_{h \rightarrow 0}\frac{\abs{f(x+h)-f(x)-f'(x)h}}{h} = 0
\]

This is better because it's easier to work with functions that go to $0$.
\[
	f(x+h)=f(x)+f'(x)h+ error
\]

the error goes to zero faster than the factor $h$. The 1st non-trivial term
in the Taylor approximation for the function $f(x)$. $f(x)+f'(x)h$ = linear
approximation for the function. Linear structure is one of our basic tools
for solving problems.

$f(x, y, z)\rightarrow$ what does the derivative mean?

\[
	\vec{v}= (x, y, z), \vec{v}_0=(x_0, y_0, z_0)
\]

\[
	f(x_0, y_0, z_0) = (\text{linear transformation})\begin{bmatrix}
		x - x_0 \\
		y - y_0 \\
		z - z_0
	\end{bmatrix}
\]

\[
	\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}, \frac{\partial f}{\partial z}\rightarrow \text{able to compute}
\]

action takes vectors to numbers. Matrices are arrays of numbers, they act on vectors
and act on vectors: a $n$ x $m$ matrix.

\begin{definition}[Gradient]
	\[
		\nabla = \begin{bmatrix}
			\frac{\partial f}{\partial x} \\
			\frac{\partial f}{\partial y} \\
			\frac{\partial f}{\partial z}
		\end{bmatrix}
	\]
\end{definition}

\subsection{Lecture--September 1, 2021}

\begin{definition}[Laplacian]
	The Laplacian is defined by:
	\[
		\sum_{i=1}^n \frac{\partial^2 f}{\partial x_i^2}
	\]

	Fundamental operator for partial differential equations. Keep in mind that
	Shrodinger equation and Maxwell's equations are partial differential equations.

	In physics $\Delta = \nabla\cdot \nabla$
\end{definition}

\begin{definition}[Deriative]
	$f:\mathbb{R}^n\rightarrow \mathbb{R}^m$--define the derivative as the existence
	of a linear transformation such that the formula holds:
	\[
		\lim_{\norm{\vec{x}-\vec{a}}\rightarrow 0}
		\frac{\norm{f(\vec{x}) - f(\vec{a}) - (Df)\vec{a}(\vec{x}-\vec{a})}_{\mathbb{R}^m}}
		{\norm{\vec{x}-\vec{a}}_{\mathbb{R}^n}} \rightarrow 0
	\]
	the linear transformation $Df$ involves data of evaluated at points:
	\[
		Df(\vec{a}) = f(\vec{a})
	\]
\end{definition}
Sometimes this can be written as $(Df)_{\vec{a}}$ or
$Df\rvert_{\vec{x}=\vec{a}}$. The transformation $Df$ takes vectors in
$\mathbb{R}^n$ to vectors in $\mathbb{R}^m$. $\vec{x}-\vec{a}$ is a column vector.
If $m=1$, $(Df)(a)(\vec{x}-\vec{a})$ corresponds to $\nabla f\cdot (\vec{x}-\vec{a})=$ numerical value.

The error term is $\text{Error}(\vec{x}, \vec{a})=\norm{f(x)-f(a)-(Df)(a)(x-a)}$

\begin{definition}[Jacobian Matrix]
	$(Df)(\vec{a})$ becomes the Jacobian matrix.

	\[
		\begin{bmatrix}
			\frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \\
			\vdots                            & \ddots & \vdots                            \\
			\frac{\partial f_m}{\partial x_1} & \cdots & \frac{\partial f_m}{\partial x_n}
		\end{bmatrix}
	\]
\end{definition}

For $n=m$ (square matrix) sometimes the \textbf{determinant of $J$ is called the Jacobian}.
Jacobian acts on vectors in $\mathbb{R}^n$.

In terms of rows:
\[
	J = \begin{bmatrix}
		\nabla f_1 \\
		\nabla f_2 \\
		\vdots     \\
		\nabla f_m
	\end{bmatrix}
\]
\[
	J(\vec{x}-\vec{a}) = \begin{bmatrix}
		\nabla f_1\cdot (\vec{x}-\vec{a}) \\
		\nabla f_2\cdot (\vec{x}-\vec{a}) \\
		\vdots                            \\
		\nabla f_m\cdot (\vec{x}-\vec{a})
	\end{bmatrix}
\]

\begin{remark}[Trace and Divergence]
	Note that the trace of $J$ is $\sum_{i=1}^{n}\frac{\partial f_i}{\partial x_i}$ which
	is also the divergence of the vector field $\nabla \cdot F$.
\end{remark}

Formula called the change of variables theorem.

$dudv=|\det J|dxdy\rightarrow $ connect with change of variables theorem.

Fact not emphasized within the text is that the determinant depends on
orientation--this is why we put absolute value.

\begin{definition}[Vector Spaces]
	In mathematics, physics, and engineering, a vector space (also called a
	linear space) is a set whose elements, often called vectors, may be added
	together and multiplied ("scaled") by numbers called scalars.
	Scalars are often real numbers, but can be complex numbers or, more
	generally, elements of any field. The operations of vector addition and
	scalar multiplication must satisfy certain requirements, called vector
	axioms. The terms real vector space and complex vector space are often used
	to specify the nature of the scalars: real coordinate space or complex coordinate space.
\end{definition}

Matrices are arrays:
\[
	\begin{bmatrix}
		a_{11} & a_{12} & \cdots & a_{1n} \\
		a_{21} & a_{22} & \cdots & a_{2n} \\
		\vdots & \vdots & \ddots & \vdots \\
		a_{m1} & a_{m2} & \cdots & a_{mn}
	\end{bmatrix}
\]

\end{document}