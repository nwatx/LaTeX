\section{January 10th, 2023}

We discussed course logistics and some basic concepts of NLP.

\begin{itemize}
	\item Sometimes NLP can mess up, e.g. ``I'm a huge metal fan'' and be
	      poorly translated to things like ``I am a huge ventilator made out of metal.''
\end{itemize}

First Attempt: Statistical Modeling of Language--The Shannon Game

\begin{itemize}
	\item How well can we predict the next word?
	\item Example: When I eat pizza, I wipe off the\dots
	\item Many children are allergic to\dots
	\item I saw a\dots
\end{itemize}

\subsection{A Brief History of NLP}

\begin{enumerate}
	\item NLP starts with writing rules in which we attempt to come up with rules that can determine the structure of a language. Obviously, this doesn't work because langugages are ambiguous and incomplete. Writing rules: \begin{itemize}
		      \item Colorless green ideas sleep furiously
		      \item Furiously sleep ideas green colorless
	      \end{itemize}
	\item \B{Annotating Data:} Penn Treebank (1993). Researchers annotatated 50,000 sentences. 40,000 training, 2400 for testing.
	\item Supervised Classification models
	\item Unsupervised topic models
	\item Semi-supervised structured prediction
	\item Neural networks
\end{enumerate}

\subsection{Current Applications}

It's interesting to note that many models today are \B{simple and expensive} and rely on the fact that they use lots of data and scale. There is limited success in integrating perception, interaction, social grounding. Symbolic reasoning reasoning, common sense reasoning, compositionally still are areas of development. The models also suffer from being incorrect. Models can also suffer from ethical concerns e.g. learning biases in incorrect data.

Currently, there is rapid progress in modern language tasks. Synthesis with other related technologies, e.g. robotics.